# Mechanisticâ€¯Interpretabilityâ€¯forâ€¯AIâ€¯Alignment (Inverse AI)
> Automated Circuit Discovery & Intervention

[![Website Status](https://img.shields.io/website?url=<YOUR_SITE_URL>)](<YOUR_SITE_URL>)
[![License](https://img.shields.io/github/license/<ORG>/<REPO>.svg)](LICENSE)
[![Last Commit](https://img.shields.io/github/last-commit/<ORG>/<REPO>.svg)](https://github.com/<ORG>/<REPO>/commits/main)

A research codeâ€‘base and public website accompanying my PhD project on **mechanistic interpretability**â€”the study of how internal circuits in large language models (LLMs) implement behaviours such as bias, truthfulness and toxicity, and how targeted interventions can make those models safer and more aligned.  
The repository hosts (a) the static site with project updates & demo notebooks, and (b) the openâ€‘source toolkit used in the papers and benchmarks.

---

## Table of Contents
1. [Project Description](#project-description)  
2. [Key Features](#key-features)  
3. [Live Demo](#live-demo)  
4. [QuickÂ Start](#quick-start)  
5. [ProjectÂ Structure](#project-structure)  
6. [Research Roadâ€‘map](#research-road-map)  
7. [Contributing](#contributing)  
8. [License](#license)  
9. [Citation](#citation)  
10. [References](#references)

---

## Project Description
Large language models excel at many tasks but remain **opaque â€œblack boxesâ€**.  
This project develops *scalable* methods to:

* discover neural circuits in Transformer models responsible for alignmentâ€‘critical behaviours;
* intervene on those circuits (e.g., activation clamping, local weight edits);
* empirically evaluate safety improvements without degrading task performance.

---

## Key Features
| Area | What it Delivers |
|------|------------------|
| **Interpretabilityâ€¯Toolkit** | Python API (built on [TransformerLens](https://github.com/TransformerLensOrg/TransformerLens)) to intercept activations, trace pathways, and visualise circuits. |
| **Automated Circuit Discovery** | Endâ€‘toâ€‘end pipeline: data probing â†’ attribution (activation patching) â†’ circuit graph export. |
| **Targeted Interventions** | Functions for feature steering, parameter patching, and postâ€‘hoc editing with unit tests. |
| **Benchmark Suite** | Scripts to measure toxicity reduction, factual recall and bias before/after interventions. |
| **Website & Docs** | Static site (built with **<TECH e.g. Docusaurus/Jekyll/Next.js>**) hosting docs, tutorials & experiment dashboards.|

---

## Live Demo
> ğŸŒ **Production site:** <https://<YOUR_SITE_URL>>

Screenshots and sample notebooks are in [`docs/assets`](docs/assets).

---

## QuickÂ Start

### Prerequisites
* PythonÂ 3.10+  
* PyTorchÂ â‰¥â€¯2.2 with CUDAÂ 11.x  
* Optional: [WeightsÂ & Biases](https://wandb.ai/) for experiment tracking  

### Installation
```bash
git clone https://github.com/<ORG>/<REPO>.git
cd <REPO>
pip install -r requirements.txt          # core + web
# or, for dev:
pip install -e ".[dev,docs]"
```

### Run the Website Locally
```bash
# example for Docusaurus
cd website
npm install
npm run start
```

### First Experiment
```bash
python scripts/discover_circuits.py        --model meta-llama/Meta-Llama-3-8B        --task toxicity
```

---

## Projectâ€¯Structure
```text
<REPO>/
â”œâ”€â”€ src/              # core interpretability library
â”‚   â”œâ”€â”€ circuits/     # discovery algorithms
â”‚   â”œâ”€â”€ interventions/
â”‚   â””â”€â”€ evaluation/
â”œâ”€â”€ website/          # static site source
â”œâ”€â”€ scripts/          # CLI entryâ€‘points
â”œâ”€â”€ models/           # (optional) cached model weights
â”œâ”€â”€ docs/             # extra papers, posters, PDF exports
â””â”€â”€ tests/
```

---

## Research Roadâ€‘map
| Phase | Goal | Status |
|-------|------|--------|
| 1. CircuitÂ Discovery | Map circuits linked to toxicity, bias, factual recall using activation patching. | *inÂ progress* |
| 2. Intervention | Develop featureâ€‘steering & weightâ€‘editing methods that preserve perplexity. | *planned* |
| 3. Largeâ€‘Scale Eval | Benchmark on multiple LLM sizes; release results & tooling. | *pending* |

---

## Contributing
1. **Fork** the repo & create a feature branch.  
2. Follow the coding style in `.editorconfig` & run `preâ€‘commit`.  
3. Open a pull request with a clear description of *why* the change matters.  
Please read [`CONTRIBUTING.md`](CONTRIBUTING.md) for full guidelines.  

---

## License
Distributed under the **<LICENSE NAME>** license. See [`LICENSE`](LICENSE) for details.

---

## Citation
If you use this code or dataset in your research, please cite:
```bibtex
@software{<YOUR_LASTNAME>_2025_mechanistic,
  author       = {<Your Name>},
  title        = {Mechanistic Interpretability for AI Alignment},
  year         = {2025},
  publisher    = {GitHub},
  url          = {https://github.com/<ORG>/<REPO>}
}
```

---

## References
1. A.â€¯Rai *etâ€¯al.*, â€œMechanistic Interpretability for Large Language Models: Techniques and Challenges,â€ *arXiv* 2407.02646,â€¯2024.  
2. **TransformerLens**â€”â€œLibrary for Mechanistic Interpretability,â€ GitHub,â€¯2023.  
3. S.â€¯Syed *etâ€¯al.*, â€œActivationâ€¯Patching: Identifying Neural Circuits,â€ *arXiv* 2309.16042,â€¯2023.  
4. AI Safety Fundamentals, â€œIntroduction to Mechanistic Interpretability,â€â€¯2023.

---

> **Note**: Replace every placeholder (anglesâ€¯`<â€¦>` or CAPS) before pushing.  
> For additional badges (e.g., GitHubÂ Actions, DOI via Zenodo), see <https://shields.io>.
